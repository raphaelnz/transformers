-------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------
# A. Preparing the data

# A.1 binarize data 
python scripts/binarized_data.py \
    --file_path data/dump_korwiki.txt \
    --tokenizer_type bert \
    --tokenizer_name bert-base-multilingual-cased \
    --dump_file data/binarized_text 
    
# A.2 token counts
python scripts/token_counts.py \
    --data_file data/binarized_text.bert-base-multilingual-cased.pickle \
    --token_counts_dump data/token_counts.bert-base-multilingual_cased.pickle \
    --vocab_size 119547


-------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------
# A~B. Student weight initialization
Tips: Starting distillated training with good initialization of the model weights is crucial to reach decent performance. I
n our experiments, we initialized our model from a few layers of the teacher (Bert) itself! Please refer to scripts/extract.py 
and scripts/extract_distilbert.py to create a valid initialization checkpoint and use --student_pretrained_weights argument 
to use this initialization for the distilled training!

python scripts/extract_distilbert.py \
    --model_type bert \
    --model_name bert-base-multilingual-cased \
    --dump_checkpoint serialization_dir/tf_bert-base-multilingual-cased_vocab_transformed.pth \
    --vocab_transform 

-------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------

# B. Training
+sanity check -clm changed

export NODE_RANK=0
export N_NODES=1
export N_GPU_NODE=2
export WORLD_SIZE=2
export MASTER_PORT=31415
export MASTER_ADDR=127.0.0.1 

pkill -f 'python -u train.py'

python -m torch.distributed.launch \
    --nproc_per_node=$N_GPU_NODE \
    --nnodes=$N_NODES \
    --node_rank $NODE_RANK \
    --master_addr $MASTER_ADDR \
    --master_port $MASTER_PORT \
    train.py \
        --force \
        --n_gpu $WORLD_SIZE \
        --student_type distilbert \ 
        --student_config training_configs/distilbert-base-multilingual-cased.json \
        --student_pretrained_weights serialization_dir/tf_bert-base-multilingual-cased_vocab_transformed.pth \ 
        --teacher_type bert \
        --teacher_name bert-base-multilingual-cased \
        --dump_path serialization_dir/my_first_training \
        --data_file data/binarized_text.bert-base-multilingual-cased.pickle \
        --token_counts data/token_counts.bert-base-multilingual_cased.pickle 
 
# -> In a single line (epoch 3)
python -m torch.distributed.launch --nproc_per_node=$N_GPU_NODE  --nnodes=$N_NODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT train.py --force --n_gpu $WORLD_SIZE --student_type distilbert --student_config training_configs/distilbert-base-multilingual-cased.json --student_pretrained_weights serialization_dir/tf_bert-base-multilingual-cased_vocab_transformed.pth --teacher_type bert --teacher_name bert-base-multilingual-cased --dump_path serialization_dir/my_first_training --data_file data/binarized_text.bert-base-multilingual-cased.pickle --token_counts data/token_counts.bert-base-multilingual_cased.pickle --alpha_ce 0.33 --alpha_mlm 0.33 --alpha_cos 0.33 --mlm

# -> In a single line (epoch 20)
python -m torch.distributed.launch --nproc_per_node=$N_GPU_NODE  --nnodes=$N_NODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT train.py --force --n_gpu $WORLD_SIZE --n_epoch 20 --student_type distilbert --student_config training_configs/distilbert-base-multilingual-cased.json --student_pretrained_weights serialization_dir/tf_bert-base-multilingual-cased_vocab_transformed.pth --teacher_type bert --teacher_name bert-base-multilingual-cased --dump_path serialization_dir/my_first_training --data_file data/binarized_text.bert-base-multilingual-cased.pickle --token_counts data/token_counts.bert-base-multilingual_cased.pickle --alpha_ce 0.33 --alpha_mlm 0.33 --alpha_cos 0.33 --mlm


# solution -> make a .sh file 
----------------------------------------------------------------------------------------------------
--------------------------  Below is updated 'pytorch-transformers' ver. ------------------------
--------------------------  FOR finetuning                                       -----------------------
----------------------------------------------------------------------------------------------------
D. Finetuning on KorQuAD

+ add run_korquad.py 
+ get rid of git part 

# distilbert_student finetuining on Linux machine
# Don't do lower CASE on Korean PLZ~! =>  cf) https://github.com/huggingface/transformers/issues/131
# Run the below command in current directory
# 20190927 : FIXME -> distributed training error! (Process halts just before actual training step) 

python -m torch.distributed.launch --nproc_per_node=2 ./run_distilbert_korquad.py \
    --model_type distilbert \
    --model_name_or_path ./distillation/serialization_dir/my_first_training/ \
    --tokenizer_name bert-base-multilingual-cased \
    --do_train \
    --do_eval \
    --train_file ./data/KorQuAD_v1.0_train.json \
    --predict_file ./data/KorQuAD_v1.0_dev.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ./distillation/models/distilBERT_KorQuAD/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   

----------------------------------------------------------------------------------------------------
# original bert fine tuning test

python -m torch.distributed.launch --nproc_per_node=2 ./run_squad.py \
    --model_type bert \
    --model_name_or_path bert-base-multilingual-cased \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file ./data/KorQuAD_v1.0_train.json \
    --predict_file ./data/KorQuAD_v1.0_dev.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ./distillation/models/distilBERT_KorQuAD/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   